#!/bin/bash
# Filename: install-local-llm.sh
# Description: One-click installer for Ollama (Local LLM) on CachyOS/Arch with GPU support.
# Usage: curl -fsSL ... | bash  OR  sudo ./install-local-llm.sh

set -e

echo "========================================="
echo "  Local LLM Installer (Ollama)"
echo "  Target: CachyOS / Arch Linux"
echo "========================================="

# 1. Check for Root
if [ "$EUID" -ne 0 ]; then 
  echo "Error: Please run with sudo."
  exit 1
fi

# 2. Detect GPU (NVIDIA)
HAS_NVIDIA=false
if lspci | grep -i nvidia >/dev/null 2>&1; then
    HAS_NVIDIA=true
    echo "[OK] NVIDIA GPU detected. Will configure for GPU acceleration."
else
    echo "[INFO] No NVIDIA GPU detected. Will run on CPU (slower)."
fi

# 3. Install Ollama
echo "[Installing Ollama...]"
if command -v ollama &> /dev/null; then
    echo "Ollama is already installed. Skipping installation."
else
    # Official install script handles Arch/CachyOS well
    curl -fsSL https://ollama.com/install.sh | sh
fi

# 4. Configure Systemd Service for GPU (If needed)
if [ "$HAS_NVIDIA" = true ]; then
    echo "[Configuring systemd for NVIDIA...]"
    # Ensure the service has access to NVIDIA devices
    # Usually the official script handles this, but we enforce it here just in case
    systemctl daemon-reload
    
    # Verify nvidia-utils are present (from your previous GPU script)
    if ! pacman -Q nvidia-utils &> /dev/null; then
        echo "Warning: nvidia-utils not found. Installing..."
        pacman -S --noconfirm nvidia-utils
    fi
fi

# 5. Start and Enable Service
echo "[Starting Ollama service...]"
systemctl enable --now ollama.service

# Wait for service to be ready
echo "Waiting for Ollama to start..."
sleep 3

# 6. Pull Default Model
# You can change 'qwen2.5:7b' to 'llama3', 'gemma2', 'mistral', etc.
DEFAULT_MODEL="qwen2.5:7b"
echo "[Pulling model: $DEFAULT_MODEL ...]"
echo "(This may take a few minutes depending on your internet speed)"

# Check if model already exists
if ollama list | grep -q "$DEFAULT_MODEL"; then
    echo "[OK] Model $DEFAULT_MODEL is already present."
else
    ollama pull $DEFAULT_MODEL
fi

# 7. Firewall Configuration (Optional but recommended for LAN access)
echo "[Checking Firewall...]"
if command -v ufw &> /dev/null; then
    if ufw status | grep -q "11434"; then
        echo "Port 11434 already open."
    else
        echo "Opening port 11434 for LAN access..."
        ufw allow 11434/tcp comment "Ollama API"
    fi
elif command -v firewall-cmd &> /dev/null; then
    if firewall-cmd --list-ports | grep -q "11434"; then
        echo "Port 11434 already open."
    else
        echo "Opening port 11434 for LAN access..."
        firewall-cmd --permanent --add-port=11434/tcp
        firewall-cmd --reload
    fi
fi

# 8. Verification
echo "========================================="
echo "  Installation Complete!"
echo "========================================="
echo "Service Status:"
systemctl status ollama --no-pager -l

echo ""
echo "Test Command:"
echo "  ollama run $DEFAULT_MODEL \"Hello, how are you?\""
echo ""
echo "API Endpoint for OpenClaw:"
echo "  http://127.0.0.1:11434/v1"
echo "  (If running OpenClaw in Podman, use: http://host.containers.internal:11434/v1)"
echo "========================================="
